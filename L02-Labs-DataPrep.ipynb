{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>Use this notebook to follow along with the lab tutorial.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Lesson 2 - Data Preparation and Retrieval</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Retrieving Data from URL, SQL Server, and Azure Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "s = requests.get(url).content\n",
    "c = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "c.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \\\n",
    "                    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \\\n",
    "                    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from SQL Server (DO NOT Run in CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER=<server URL>; DATABASE=<database name>; UID=<user id>;PWD= <pwd>')\n",
    "cursor = cnxn.cursor()\n",
    "cursor.execute(\"SELECT WORK_ORDER.TYPE,WORK_ORDER.STATUS, WORK_ORDER.BASE_ID, WORK_ORDER.LOT_ID FROM WORK_ORDER\")\n",
    "for row in cursor.fetchall():\n",
    "    print row\n",
    "cursor.close()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the storage account key:········\n",
      "Blob names in container are:\n",
      "RetailChurnData.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Address</th>\n",
       "      <th>Gender</th>\n",
       "      <th>UserType</th>\n",
       "      <th>Column 0</th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Value</th>\n",
       "      <th>Location</th>\n",
       "      <th>ProductCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2105345</td>\n",
       "      <td>D</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>220245</td>\n",
       "      <td>1215553</td>\n",
       "      <td>12/31/2000 12:00:00 AM</td>\n",
       "      <td>4710040000000</td>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2105345</td>\n",
       "      <td>D</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>220246</td>\n",
       "      <td>1216545</td>\n",
       "      <td>12/31/2000 12:00:00 AM</td>\n",
       "      <td>4711090000000</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2105345</td>\n",
       "      <td>D</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>220247</td>\n",
       "      <td>1216590</td>\n",
       "      <td>12/31/2000 12:00:00 AM</td>\n",
       "      <td>9556000000000</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2105345</td>\n",
       "      <td>D</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>220248</td>\n",
       "      <td>1217249</td>\n",
       "      <td>12/31/2000 12:00:00 AM</td>\n",
       "      <td>4711800000000</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2105345</td>\n",
       "      <td>D</td>\n",
       "      <td>F</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>220249</td>\n",
       "      <td>1217259</td>\n",
       "      <td>12/31/2000 12:00:00 AM</td>\n",
       "      <td>4710030000000</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserId Age Address   Gender UserType  Column 0  TransactionId  \\\n",
       "0  2105345   D       F  Unknown  Unknown    220245        1215553   \n",
       "1  2105345   D       F  Unknown  Unknown    220246        1216545   \n",
       "2  2105345   D       F  Unknown  Unknown    220247        1216590   \n",
       "3  2105345   D       F  Unknown  Unknown    220248        1217249   \n",
       "4  2105345   D       F  Unknown  Unknown    220249        1217259   \n",
       "\n",
       "                Timestamp         ItemId  Quantity  Value Location  \\\n",
       "0  12/31/2000 12:00:00 AM  4710040000000         1    149  Unknown   \n",
       "1  12/31/2000 12:00:00 AM  4711090000000         1    179  Unknown   \n",
       "2  12/31/2000 12:00:00 AM  9556000000000         1     28  Unknown   \n",
       "3  12/31/2000 12:00:00 AM  4711800000000         1    199  Unknown   \n",
       "4  12/31/2000 12:00:00 AM  4710030000000         1    139  Unknown   \n",
       "\n",
       "  ProductCategory  \n",
       "0         Unknown  \n",
       "1         Unknown  \n",
       "2         Unknown  \n",
       "3         Unknown  \n",
       "4         Unknown  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.storage.blob import BlockBlobService\n",
    "blob_account_name = \"uwcoursestorage\"\n",
    "import getpass\n",
    "storage_key = getpass.getpass(\"Please input the storage account key:\")\n",
    "block_blob_service = BlockBlobService(account_name=blob_account_name, account_key=storage_key)\n",
    "generator = block_blob_service.list_blobs('testdata')\n",
    "print(\"Blob names in container are:\")\n",
    "for blob in generator:\n",
    "    print(blob.name)\n",
    "block_blob_service.get_blob_to_path('testdata', 'RetailChurnData.csv', 'RetailChurnData.csv')\n",
    "\n",
    "df = pd.read_csv(\"RetailChurnData.csv\", header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Data Frames\n",
    "Appending to the same column using the `concat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], \\\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'], \\\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'], \\\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']}, \\\n",
    "                   index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'], \\\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'], \\\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'], \\\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\\\n",
    "                   index=[4, 5, 6, 7]) \n",
    "\n",
    "frames = [df1, df2]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join two Data Frames\n",
    "Using the `merge` function to join two dataframes so that they are one dtataframe with both sets of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], \\\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'], \\\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], \\\n",
    "                      'C': ['C0', 'C1', 'C2', 'C3'], \\\n",
    "                      'D': ['D0', 'D1', 'D2', 'D3']})\n",
    "\n",
    "\n",
    "result = pd.merge(left, right, on='key')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge is an inner join by default.\n",
    "left has K2 but right1 does not have K2, when we join we will drop row K2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right1 = pd.DataFrame({'key': ['K0', 'K1', 'K3'], \\\n",
    "                      'C': ['C0', 'C1', 'C3'], \\\n",
    "                      'D': ['D0', 'D1', 'D3']})\n",
    "result1 = pd.merge(left, right1, on='key') # Inner join by default\n",
    "result1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge can be left outer join by using the \"how\" parameter.\n",
    "Recall right1 does not have K2, what do you expect as the result2? result3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.merge(left, right1, how='left', on='key') # left outer join\n",
    "result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = pd.merge(left, right1, how='right', on='key') # right outer join\n",
    "result3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge can also do a full outer join by \"how=outer\"\n",
    "left1 does not have K0, what do you expect as the result4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1 = pd.DataFrame({'key': ['K1', 'K2', 'K3'], \\\n",
    "                     'A': ['A1', 'A2', 'A3'], \\\n",
    "                     'B': ['B1', 'B2', 'B3']})\n",
    "result4 = pd.merge(left1, right1, how='outer', on='key') # outer join\n",
    "result4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from Local CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Bank Data.csv\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv(file, header=0)\n",
    "print(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the General Description of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error in the next cell block, uncomment the following line to install pandas_profiling\n",
    "# !pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Handle Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = [\"SEPAL_LENGTH\", \"SEPAL_WIDTH\", \"PETAL_LENGTH\", \"PETAL_WIDTH\"]\n",
    "X_incomplete = X.copy()\n",
    "X_incomplete.loc[150] = [np.NaN, np.NaN, np.NaN, np.NaN]\n",
    "\n",
    "X_incomplete.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_incomplete1 = X_incomplete.copy()\n",
    "X_incomplete1.dropna(inplace=True)\n",
    "X_incomplete1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation: Replace Missing Values with Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_incomplete1 = X_incomplete.copy()\n",
    "X_incomplete1.fillna(X_incomplete1.mean(), inplace=True)\n",
    "# count the number of NaN values in each column\n",
    "X_incomplete1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation: Use Scikit-Learn Preprocessing\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "\n",
    "- Can use mean, median, most frequent for the \"strategy\".\n",
    "- This method can also impute rowwise as well as columnwise (cols: axis 0 vs. rows: axis 1).\n",
    "- Works for numeric and categorical measures because of \"most frequent\" strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_incomplete1 = X_incomplete.copy()\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\") #default is mean\n",
    "transformed_values = pd.DataFrame(imputer.fit_transform(X_incomplete1))\n",
    "transformed_values.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [92,19,101,58,1053,91,26,78,10,13,-40,101,86,85,15,89,89,28,-5,41]\n",
    "print(\"mean=%.2f\"%np.mean(X))\n",
    "from scipy.stats import mstats\n",
    "X1 = mstats.winsorize(X, limits=0.05, inplace=True) #It is thresholding at 5th and 95th percentiles\n",
    "print(X1)\n",
    "print(\"After winsorizing, mean=%.2f\"%np.mean(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics Robust to Outliers\n",
    "Impact of outliers on statistics and how we can mitigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 2 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "import matplotlib.pyplot as plt\n",
    "count, bins, ignored = plt.hist(s, 30, normed=True)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r')\n",
    "plt.show()\n",
    "print(\"mean=%.2f, sd=%.2f\"%(np.mean(s), np.std(s)))\n",
    "\n",
    "# Introduce an outlier\n",
    "s[983] = 200\n",
    "print(\"with outlier, mean=%.2f, sd=%.2f\"%(np.mean(s), np.std(s)))\n",
    "\n",
    "from statsmodels import robust\n",
    "print(\"with outlier, median=%.2f, sd=%.2f\"%(np.median(s), 1.4826*robust.mad(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels import robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Scaling, Binning, and Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale to between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_minmax = df.copy()\n",
    "data_minmax.iloc[:,[0,3,5]] = min_max_scaler.fit_transform(data_minmax.iloc[:,[0,3,5]])\n",
    "data_minmax.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### z-transaction\n",
    "Shifting the decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data_z = df.copy()\n",
    "data_z.iloc[:,[0,3,5]] = preprocessing.scale(data_z.iloc[:,[0,3,5]])\n",
    "data_z.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning the Continuous Variable into Bins\n",
    "Using the Adult Income Data as an example. Data has been read into data frame c in the beginning of this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins = np.linspace(15,70,12) #Generating a sequence with 12 numbers, step size 5\n",
    "print(bins) #print the bins definition\n",
    "age_binned = pd.cut(c.iloc[:,0], bins, right=True, labels=range(11)) # put the age into bins\n",
    "print(\"Original value=%d, bin label=%d\"%(c.iloc[1,0], age_binned[1]))\n",
    "print(\"Original value=%d, bin label=%d\"%(c.iloc[2,0], age_binned[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Time Stamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeStamps in Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestring = \"2017/11/06 19:00:13\" # A Monday\n",
    "from datetime import datetime\n",
    "\n",
    "datetime_object = datetime.strptime(datestring, '%Y/%m/%d %H:%M:%S')\n",
    "print(\"Year=%d\"%datetime_object.year)\n",
    "print(\"Month=%d\"%datetime_object.month)\n",
    "print(\"Day of Month=%d\"%datetime_object.day)\n",
    "print(\"Weekday=%d\"%datetime_object.weekday())\n",
    "print(\"Week Number=%s\"%datetime_object.strftime(\"%U\"))\n",
    "print(\"Hour=%d\"%datetime_object.hour)\n",
    "print(\"Minute=%d\"%datetime_object.minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Unix Time Stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1352068320\n",
    "datetime_object = datetime.fromtimestamp(ts)\n",
    "print(datetime_object)\n",
    "print(\"Year=%d\"%datetime_object.year)\n",
    "print(\"Month=%d\"%datetime_object.month)\n",
    "print(\"Day of Month=%d\"%datetime_object.day)\n",
    "print(\"Weekday=%d\"%datetime_object.weekday())\n",
    "print(\"Week Number=%s\"%datetime_object.strftime(\"%U\"))\n",
    "print(\"Hour=%d\"%datetime_object.hour)\n",
    "print(\"Minute=%d\"%datetime_object.minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5; padding: 15px 5px; \" >\n",
    "<p>For additional practice, please see the Workshop notebooks.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
